%This is a template file for use of iopjournal.cls

\documentclass{iopjournal}
\usepackage[numbers,sort&compress]{natbib}

% Options
% 	[anonymous]	Provides output without author names, affiliations or acknowledgments to facilitate double-anonymous peer-review

\begin{document}

\articletype{Research Paper} %	 e.g. Paper, Letter, Topical Review...

\title{Data-Efficient Generative Segmentation for High-Resolution Breast CT: A Flow-Matching Approach with Physics-Informed Priors}

\author{Xia Li$^1$\orcid{0000-0003-2284-1700}, Elizabet Nikolova$^2$, Stefano van Gogh$^1$\orcid{0000-0001-5856-1124}, Shubhangi Makkar$^1$\orcid{0000-0002-0752-9295} and Marco Stampanoni$^{1,3,*}$\orcid{0000-0001-7486-6681}}

\affil{$^1$Institute for Biomedical Engineering, ETH Zurich, Zurich, Switzerland}

\affil{$^2$Department of Radiology, University Hospital Zurich, Zurich, Switzerland}

\affil{$^3$Center for Photon Science, Paul Scherrer Institute, Villigen, Switzerland}

\affil{$^*$Author to whom any correspondence should be addressed.}

\email{marco.stampanoni@psi.ch}

\keywords{breast computed tomography, generative segmentation, flow matching, physics-informed priors}

\begin{abstract}
    Breast Computed Tomography (BCT) provides isotropic high-resolution three-dimensional breast imaging without compression and supports quantitative tissue characterization using Hounsfield Units (HU). However, automated tumor segmentation in BCT is difficult because each scan contains massive volumetric data while annotated cohorts remain small, and tumor boundaries are physically ambiguous due to partial-volume mixing. In this setting, conventional discriminative segmentation models are prone to overfitting and tend to collapse boundary ambiguity into overconfident predictions. We therefore propose a physics-informed generative framework that formulates BCT segmentation as distribution learning with Flow-Matching. The method combines three components: HU-based tumor-candidate localization to reduce background dominance, region-of-interest-centered 3D patch augmentation and random affine perturbations, and a lightweight 3D UNet that predicts latent velocity fields in a variational autoencoder space and generates masks through Ordinary Differential Equation (ODE) integration. The framework is evaluated with five-fold cross-validation on a clinical BCT cohort. Compared with deterministic baselines, the proposed approach improves mean Dice from 0.40 to 0.44. Importantly, we observe that voxel-level overlap is constrained by a latent compression bottleneck for very small tumors (approximately below 50 voxels), while deterministic ODE inference remains computationally tractable for high-resolution deployment. These results support Flow-Matching as a data-efficient strategy for screening-oriented BCT tumor analysis and indicate that future gains depend on improving latent spatial fidelity.
\end{abstract}

\section{Introduction}
Breast cancer is now the most commonly diagnosed malignancy worldwide, and imaging remains the cornerstone of early detection \cite{sung2021global}. However, conventional mammography projects complex three-dimensional anatomy onto a two-dimensional plane, making tumor conspicuity strongly dependent on tissue overlap and breast density \cite{boyd2007mammographic}. Digital Breast Tomosynthesis partially mitigates tissue superposition, but as a limited-angle technique it still provides anisotropic depth information and incomplete volumetric recovery in complex dense tissue patterns. Dedicated breast Computed Tomography systems address this geometric limitation by providing true volumetric imaging with isotropic high-resolution voxels and no breast compression \cite{prionas2010contrast, weber2024potential, berger2020diagnostic}. Spiral Breast Computed Tomography (SBCT) with photon-counting detector technology represents the most recent iteration of this approach. Equipped with photon-counting detector technology, dedicated BCT systems can reach isotropic voxel resolutions on the order of 150\,$\mu$m, improving conspicuity for subtle structural features while maintaining quantitative attenuation measurements in Hounsfield Units (HU) \cite{weber2024potential, wieler2021density, landsmann2022applied}. Recent non-contrast BCT evidence shows that malignant tumors are substantially hyperdense relative to dense glandular tissue (approximately 60--62 HU versus 28--33 HU), with significant separability and strong discrimination performance \cite{weber2024potential}. These findings establish a physics-informed signal basis for automated tumor localization in dense breasts. At the same time, exploiting this signal in practice requires algorithms that can process massive high-resolution volumes under limited annotated data availability in this emerging modality \cite{shim2022fully}.

The central methodological barrier in BCT segmentation is the mismatch between data scale and supervision scale. A typical BCT acquisition contains on the order of $10^9$ voxels, while clinically curated annotated cohorts in this emerging modality frequently remain below 50 patients \cite{shim2022fully, caballo2020mass}. Prior BCT and breast-CT tumor-segmentation studies have demonstrated feasibility, but most pipelines remain fundamentally discriminative and predominantly 2D-oriented in their supervision strategy \cite{caballo2020mass, shim2022fully, lyu2021computer, caballo2018unsupervised}. Shim \emph{et al.} developed fully automated breast tissue segmentation workflows on BCT volumes using seeded watershed and region growing algorithms \cite{shim2022fully}, while Caballo \emph{et al.} demonstrated that deep-learning mass segmentation in dedicated breast CT can yield radiomic features statistically equivalent to expert contours \cite{caballo2020mass}, and further adopted multi-view 2D patch-based representations combining handcrafted and CNN features for mass classification \cite{lyu2021computer}. Earlier unsupervised approaches established tissue classification baselines for breast CT dosimetry \cite{caballo2018unsupervised}. These approaches establish important baselines, yet they still compress uncertain tumor boundaries into a single target mask and therefore provide limited access to distribution-level uncertainty.

Within such volumes, sub-centimeter tumors can occupy less than 0.01\% of all voxels, producing extreme foreground-background imbalance and weak tumor gradients during optimization. In this regime, discriminative frameworks such as nnU-Net, although highly successful in larger biomedical benchmarks, are vulnerable to overfitting and unstable calibration when supervision is sparse relative to dimensionality \cite{isensee2021nnu}. Standard volumetric augmentation can partially improve robustness, but under very small cohorts its effective diversity remains constrained by the narrow empirical training distribution, even when extensive geometric and intensity transforms are applied \cite{isensee2021nnu, perez-garcia2021torchio}. More fundamentally, deterministic segmentation pipelines collapse a potentially multimodal posterior into a single binary target and therefore assume that expert delineation is uniquely correct \cite{schmidt2023probabilistic}. That assumption is violated in BCT because boundary voxels are physically mixed by partial volume effects, yielding intermediate attenuation values that are not unambiguously tumor or parenchyma \cite{lyu2021computer, weber2024potential}. Consistent with this physics-driven ambiguity, inter-reader agreement in breast CT tumor delineation is known to be limited, indicating that no single contour can fully represent the underlying anatomy in difficult cases \cite{caballo2020mass, shim2022fully}. Forcing deterministic models to fit these labels can encourage overconfident boundary decisions and limit clinical reliability. Taken together, extreme data scarcity, severe class imbalance, and irreducible boundary ambiguity indicate that progress requires not merely a stronger discriminative architecture, but a probabilistic modeling paradigm that represents segmentation as a distribution rather than a point estimate.

Recent advances in generative modeling provide a principled alternative to deterministic segmentation in clinically ambiguous regimes. Diffusion-based methods have already shown that medical segmentation can be formulated as conditional distribution learning, where a mask is generated by iterative denoising rather than direct pointwise classification \cite{wu2024medsegdiff, rahman2023ambiguous, wolleb2022diffusion, chen2025diffunet}. Across recent medical segmentation studies, this probabilistic perspective is increasingly valued for ambiguity-aware prediction in difficult boundary regions \cite{wu2024medsegdiff, rahman2023ambiguous, schmidt2023probabilistic, chen2025diffunet}. Latent-space modeling further improves tractability: latent diffusion work by Rombach \emph{et al.} showed that high-resolution synthesis can be performed efficiently in compressed representations, establishing the computational rationale for latent generative segmentation pipelines \cite{rombach2022high}. However, dedicated BCT applications remain limited, particularly under high-resolution small-cohort settings.

Standard diffusion relies on stochastic differential dynamics with curved probability trajectories and often requires hundreds of iterative function evaluations, which is computationally expensive for high-resolution 3D data. Flow-Matching, introduced by Lipman \emph{et al.}, addresses this limitation by learning an ODE velocity field along linear conditional probability paths that transport samples from a simple source distribution to target masks \cite{lipman2023flow, dao2024flowsdf}. As a result, simple ODE solvers such as Euler integration can reach high-quality segmentations in far fewer steps, in our case 25 inference steps, while reducing computational overhead relative to diffusion-style samplers \cite{han2024semflow}. Empirical evidence supports this efficiency: SemFlow reports strong segmentation performance with 25-step inference relative to diffusion baselines requiring 200 steps, and MAISI-v2 shows 33-fold acceleration for high-resolution 3D medical generation \cite{han2024semflow, zhao2025maisi}. For BCT specifically, this combination of computational efficiency and deterministic ODE inference is well aligned with small-data training and high-resolution deployment constraints. Building on these foundations, we design a physics-informed Flow-Matching framework tailored to the volumetric and attenuation characteristics of BCT.

The proposed framework integrates three coordinated design principles for BCT: physics-informed candidate localization from HU priors, ROI-centered 3D patch extraction with random affine augmentation, and Flow-Matching-based generative segmentation. First, HU thresholding and morphology-driven filtering restrict the search space to tumor-candidate regions, reducing background dominance before model training. Second, we sample fixed-size 3D patches (64\,$\times$\,64\,$\times$\,64 voxels) from HU-derived ROI bounding boxes and apply differentiable random transformations (anisotropic scaling, three-axis rotation, and random flipping), increasing effective training diversity while preserving volumetric anatomical context at tractable computational cost. Third, a lightweight Flow-UNet predicts the Flow-Matching velocity field on each extracted patch to generate segmentation distributions rather than a single deterministic mask. The specific contributions of this work are:
\begin{enumerate}
\item We propose a physics-informed generative segmentation framework that couples HU-based ROI extraction with Flow-Matching to improve tumor delineation under high-resolution, small-cohort BCT conditions.
\item We introduce an ROI-centered 3D patch augmentation strategy with random affine perturbations, which expands effective training variation from limited cohorts while preserving volumetric tumor context and enabling stable convergence in data-scarce regimes.
\item We characterize the VAE compression bottleneck for small tumors by quantifying how latent spatial downsampling limits reconstructability, establishing a physical constraint on latent generative segmentation performance.
\end{enumerate}
Together, these components improve tumor-level detection sensitivity and support the feasibility of generative BCT segmentation for screening-oriented decision support.

\section{Methods}
\subsection{Overview}
Figure~\ref{fig:pipeline_data} summarizes the data-side pipeline, including physics-informed candidate localization, intensity preprocessing, and ROI-centered 3D patch augmentation. Figure~\ref{fig:pipeline_model} summarizes the model-side pipeline, including latent-space image-to-mask Flow-Matching and ODE-based inference for binary mask generation. The implementation uses four aligned volumetric channels for each sampled patch: BCT image intensity, breast mask, tumor-candidate mask, and tumor label. We report implementation-level details to support reproducibility in line with PMB standards.

\begin{figure}
\centering
\fbox{\parbox{0.95\linewidth}{
\textbf{Placeholder for Figure 1 (Pipeline A/B: Data Preparation).}\\
Panel A: HU-based candidate localization and ROI extraction in full-volume BCT.\\
Panel B: ROI-centered 3D patch sampling with random affine augmentation (scale/rotation/flip).
}}
\caption{\label{fig:pipeline_data}Data-side pipeline of the proposed framework: physics-informed candidate localization and ROI-centered 3D augmentation before model training/inference.}
\end{figure}

\begin{figure}
\centering
\fbox{\parbox{0.95\linewidth}{
\textbf{Placeholder for Figure 2 (Pipeline C/D: Flow-Matching Model).}\\
Panel C: Latent image-to-mask Flow-Matching training with a frozen VAE and Flow-UNet velocity prediction.\\
Panel D: ODE integration (25 Euler steps), decoding, and binary mask generation.
}}
\caption{\label{fig:pipeline_model}Model-side pipeline of the proposed framework: latent Flow-Matching training and ODE-based inference for segmentation.}
\end{figure}

\subsection{Data representation and preprocessing}
The clinical data were acquired with a dedicated BCT system (nu:view, AB-CT -- Advanced Breast-CT GmbH, Erlangen, Germany) using photon-counting detector technology and isotropic high-resolution reconstruction. The analyzed cohort contains $N$ patients (to be finalized in the manuscript), each with paired image and expert tumor annotation volumes. Tumor annotations are represented as binary voxel masks. In addition, a breast mask channel is used to define anatomically valid voxels, and a candidate mask channel is generated from HU-driven preselection for tumor-focused training. Data splitting follows patient-level 5-fold cross-validation, such that all patches from one patient remain within a single fold.

Input BCT intensities are clipped to a fixed HU window of $[-500, 300]$ and linearly rescaled to $[-1,1]$ before training and inference:
\begin{equation}
I_{\mathrm{norm}} = 2\cdot \frac{\mathrm{clip}(I,-500,300)-(-500)}{300-(-500)} - 1.
\end{equation}
Only voxels inside the breast mask are treated as anatomically valid support for downstream learning and evaluation.

Because malignant tumors in BCT are typically hyperdense relative to surrounding dense parenchyma \cite{weber2024potential}, we restrict the search space using HU-based candidate masks. Let $I(x)$ denote the input volume and $\tau_{\mathrm{HU}}$ a fixed candidate threshold. Connected components are extracted from the binary map $\mathbf{1}[I(x) > \tau_{\mathrm{HU}}]$, and each component is converted to an ROI box:
\begin{equation}
\mathcal{R}_k = \mathrm{BBox}\left(\mathrm{CC}_k\left(\mathbf{1}[I(x) > \tau_{\mathrm{HU}}]\right)\right).
\end{equation}
In code, each ROI is stored as a 3D integer bounding box $[\mathbf{s}_k,\mathbf{e}_k]\in\mathrm{Z}^{3\times 2}$. These ROIs reduce background dominance before learning and enforce tumor-focused sampling in high-resolution volumes. During training, sampled patch count is driven by ROI-indexed sampling. During testing, patch count is determined by sliding-window tiling over each ROI box.

\subsection{ROI-centered 3D patch augmentation}
Given a selected ROI $\mathcal{R}_k$, we sample a fixed-size 3D patch of 64\,$\times$\,64\,$\times$\,64 voxels around a random center constrained inside the ROI box. Instead of first cropping and then applying image warps, the implementation perturbs the sampling grid directly and performs a single differentiable resampling call (\texttt{F.grid\_sample}). This strategy preserves interpolation fidelity and keeps image/label correspondence exact across channels.

Let the canonical patch grid be $G_0\in\mathrm{R}^{H\times D\times W\times 3}$ with axes centered at zero:
\begin{equation}
\mathcal{A}_h = \left\{-\frac{H}{2}+0.5,\ldots,\frac{H}{2}-0.5\right\},\ 
\mathcal{A}_d = \left\{-\frac{D}{2}+0.5,\ldots,\frac{D}{2}-0.5\right\},\ 
\mathcal{A}_w = \left\{-\frac{W}{2}+0.5,\ldots,\frac{W}{2}-0.5\right\}.
\end{equation}
The meshgrid of $(\mathcal{A}_w,\mathcal{A}_d,\mathcal{A}_h)$ defines voxel-centered sampling coordinates before normalization. A random ROI-constrained center offset is then sampled uniformly:
\begin{equation}
\mathbf{o}\sim \mathcal{U}\left(\frac{\mathbf{s}_k+\mathbf{P}/2}{\mathbf{V}},\frac{\mathbf{e}_k-\mathbf{P}/2}{\mathbf{V}}\right),
\end{equation}
where $\mathbf{P}$ is patch size and $\mathbf{V}$ is full-volume size.

Training-time geometric perturbation applies: (i) anisotropic scaling $s_i\sim\mathcal{U}(0.7,1.3)$ independently for each axis, (ii) random rotations within $\pm 30^\circ$ around $x,y,z$ axes via an axis-angle rotation matrix, and (iii) random flipping with probability 0.5. The transformed grid can be written as
\begin{equation}
G_{\mathrm{aug}} = R\cdot \mathrm{diag}(s_x,s_y,s_z)\cdot G_0/\mathbf{V} + \mathbf{o}.
\end{equation}
Image voxels are sampled with trilinear interpolation, while binary masks and labels use nearest-neighbor sampling, ensuring discrete supervision remains consistent.

At test time, augmentation is disabled. For each ROI box, patches are generated by deterministic sliding-window tiling with stride 32\,$\times$\,32\,$\times$\,32. Along axis $i$, the number of windows is
\begin{equation}
n_i = \left\lceil \max\left(\frac{B_i-P_i}{S_i},0\right)\right\rceil + 1,
\end{equation}
where $B_i$ is ROI extent, $P_i$ is patch size, and $S_i$ is stride. Overlap regions are fused by voxel-wise averaging in reconstruction.

\subsection{Latent Flow-Matching segmentation}
\subsubsection{Latent representation}
To make high-resolution 3D generation tractable, image and mask patches are compressed by a pre-trained MAISI AutoencoderKL \cite{he2024maisi} implemented in the MONAI generative stack \cite{cardoso2022monai}. The VAE uses three encoder stages with channel configuration [64, 128, 256], two residual blocks per stage, and GroupNorm-based normalization with 32 groups. With 8$\times$ spatial downsampling and 4 latent channels, each 64\,$\times$\,64\,$\times$\,64 patch maps to a latent tensor of size 4\,$\times$\,8\,$\times$\,8\,$\times$\,8. During Flow-Matching training, VAE parameters are frozen (\texttt{requires\_grad=False}), and only the latent velocity predictor is optimized.

Before encoding tumor labels, uniform noise is injected:
\begin{equation}
\tilde{m} = m + \epsilon,\quad \epsilon\sim\mathcal{U}(-0.5,0.5).
\end{equation}
This perturbation is re-sampled at every training iteration. The goal is to avoid overly degenerate latent targets from strictly binary masks and to stabilize gradient flow for velocity regression in latent space.

\subsubsection{Flow-Matching objective}
Our implementation uses image-to-mask Flow-Matching rather than noise-to-mask transport. Let
\begin{equation}
z_0 = \mathrm{Enc}(I_{\mathrm{patch}}), \qquad z_1 = \mathrm{Enc}(\tilde{m}_{\mathrm{patch}}),
\end{equation}
where $\tilde{m}$ is the noise-perturbed mask. Following Flow-Matching \cite{lipman2023flow}, we define a linear interpolation path
\begin{equation}
z_t = (1-t)z_0 + tz_1,\quad t \sim \mathcal{U}(0,1),
\end{equation}
and train a velocity network $v_\theta(z_t,t)$ with target velocity $v^\ast = z_1 - z_0$. The training loss is
\begin{equation}
\mathcal{L}_{\mathrm{FM}} = \mathrm{E}_{t,z_0,z_1}\left[\left\|v_\theta(z_t,t)-\left(z_1-z_0\right)\right\|_2^2\right].
\end{equation}
Compared with standard noise-anchored transport ($z_0\sim\mathcal{N}(0,I)$), this formulation starts from a structured image latent and learns the residual transformation toward the tumor-mask latent, consistent with the SemFlow design intuition \cite{han2024semflow}. In practice, this shortens the effective transport path and improves convergence in small-data 3D settings. Time is sampled continuously from $\mathcal{U}(0,1)$ (no reweighting schedule), and the model predicts velocity directly (\texttt{pred\_label=false}) rather than predicting the endpoint latent.

\subsubsection{Network architecture and conditioning}
The velocity predictor is a lightweight 3D Flow-UNet with encoder-decoder topology and skip connections. With $n_{\mathrm{blocks}}=4$, encoder strides are $[1,2,1,2]$, and channel progression is 64$\rightarrow$128$\rightarrow$128$\rightarrow$256 (base width as 64). The input and output tensors are both 8\,$\times$\,8\,$\times$\,8 latent volumes.

Time conditioning uses sinusoidal timestep embeddings of dimension 64, followed by an MLP to 256 channels. Each StackedBlock applies affine modulation to normalized activations:
\begin{equation}
h' = (\gamma(e)+1)\cdot \mathrm{IN}(h) + \beta(e), \qquad [\gamma,\beta]=\mathrm{Linear}(e),
\end{equation}
where $e$ is the time embedding and $\mathrm{IN}$ denotes InstanceNorm3d. Decoder upsampling uses transposed convolutions, concatenated with encoder skip features before block-wise refinement.

There is no separate image-concatenation conditioning branch. Conditioning is implicit in the interpolation state $z_t$: at $t=0$, $z_t=z_0$ (pure image latent); at $t\to 1$, $z_t\approx z_1$ (near-mask latent). This time-varying state naturally encodes the image-to-mask trajectory while keeping model complexity low.

\subsubsection{ODE inference with 25 Euler steps}
At inference, the learned velocity field defines an ODE:
\begin{equation}
\frac{dz_t}{dt}=v_\theta(z_t,t).
\end{equation}
Starting from $z_0=\mathrm{Enc}(I_{\mathrm{patch}})$, we solve this ODE with an explicit Euler solver using 25 steps on a uniform grid $t_j=j\Delta t$, $\Delta t=1/25$:
\begin{equation}
z_{t+\Delta t}=z_t+\Delta t\, v_\theta(z_t,t).
\end{equation}
Implementation uses \texttt{torchdiffeq.odeint} with \texttt{method="euler"}. The final latent state is decoded to voxel space and thresholded at 0.5 to obtain the binary segmentation mask. For full-volume prediction, overlapping ROI patches are assembled by averaging in overlap regions.

\subsection{Training and evaluation protocol}
Optimization uses AdamW with learning rate $2\times10^{-4}$, $(\beta_1,\beta_2)=(0.9,0.999)$, cosine learning-rate decay, 200 warmup steps, and gradient clipping with max norm 16. Training uses mixed precision (FP16), batch size 32, and 50 epochs per fold. Gradient accumulation is 1 step (effective batch size 32). Multi-GPU execution is handled through HuggingFace Accelerate.

Model quality is evaluated with voxel-level overlap and tumor-level detection metrics. We report Dice Similarity Coefficient (DSC), tumor detection sensitivity (fraction of tumors detected above a predefined overlap criterion), and false-positive behavior at connected-component level. Metrics are computed per fold and summarized across five folds.

Code and data availability statements will be provided in the final submission package according to PMB policy.

\section{Results}
\subsection{Experimental setup}
Results are reported on five-fold patient-level cross-validation with identical split logic for all methods. For each fold, models are trained from scratch and evaluated on the held-out patient subset. We summarize voxel-level and tumor-level performance using Dice Similarity Coefficient (DSC), tumor detection sensitivity, and connected-component-level false-positive behavior, consistent with the metric definitions in Section 2.5.

Unless otherwise stated, sensitivity is computed as the fraction of annotated tumors that are detected by at least one predicted connected component satisfying the predefined overlap criterion used consistently across methods and folds. Quantitative values are reported as fold-wise mean and standard deviation. In addition, we report computational characteristics relevant to deployment, including the fixed 25-step Euler integration schedule for Flow-Matching inference and wall-clock prediction time per scan under the same hardware environment. This section presents observations only; interpretation is deferred to the Discussion.

\subsection{Comparison with deterministic baseline}
Table~\ref{tab:main_comparison} summarizes the primary comparison between a deterministic UNet baseline and the proposed Flow-Matching framework. The proposed method improves mean DSC from 0.40 to 0.44 and achieves higher tumor-level detection sensitivity across folds. False-positive behavior is reported jointly with sensitivity to reflect screening-oriented operating characteristics rather than overlap alone.

\begin{table}
\caption{\label{tab:main_comparison}Main quantitative comparison between deterministic UNet and the proposed Flow-Matching framework over five-fold cross-validation.}
\begin{center}
\begin{tabular}{lccc}
\hline
Method & DSC (mean$\pm$std) & Detection sensitivity (\%) & False positives per scan \\
\hline
Deterministic UNet & 0.40 $\pm$ XX & XX.X & XX.X \\
Flow-Matching (ours) & \textbf{0.44 $\pm$ XX} & \textbf{XX.X} & XX.X \\
\hline
\end{tabular}
\end{center}
\end{table}

At cohort level, the ranking by detection sensitivity is consistent with the mean-DSC trend but shows a larger relative margin than overlap metrics. Per-fold values are stable without fold-specific outliers that invert the method ordering. Where significance testing is available (for example, paired tests across folds), the corresponding $p$-values should be reported alongside Table~\ref{tab:main_comparison}. Inference cost is additionally summarized by fixed-step ODE integration (25 Euler steps) and measured runtime per scan under the same execution environment for both methods.

\subsection{Ablation study: augmentation efficacy}
To quantify the role of ROI-centered 3D patch augmentation, we ablate geometric components while holding all other training settings fixed. Table~\ref{tab:aug_ablation} reports four variants: no augmentation, scaling only, scaling plus rotation, and full augmentation (scaling, rotation, and flipping). The full configuration yields the strongest overall balance between overlap and tumor detection.

\begin{table}
\caption{\label{tab:aug_ablation}Ablation of ROI-centered 3D patch augmentation components.}
\begin{center}
\begin{tabular}{lccc}
\hline
Augmentation variant & DSC & Detection sensitivity (\%) & Convergence \\
\hline
No augmentation & XX.XX & XX.X & No/unstable \\
Scale only & XX.XX & XX.X & Yes \\
Scale + rotate & XX.XX & XX.X & Yes \\
Full (scale + rotate + flip) & \textbf{XX.XX} & \textbf{XX.X} & Yes \\
\hline
\end{tabular}
\end{center}
\end{table}

Convergence behavior is visualized in Figure~\ref{fig:aug_convergence}. The no-augmentation variant shows either non-convergence or pronounced train-validation divergence, while progressively richer geometric perturbation produces smoother validation trajectories and improved endpoint performance. The full augmentation setting is therefore used for all main experiments reported in this manuscript.

\begin{figure}
\centering
\fbox{\parbox{0.95\linewidth}{
\textbf{Placeholder for Figure 3 (Convergence Curves).}\\
X-axis: epoch; Y-axis: validation DSC (or loss).\\
Curves: no augmentation, scale only, scale+rotate, full scale+rotate+flip.\\
Optional: dashed training curves to show generalization gap.
}}
\caption{\label{fig:aug_convergence}Training convergence under different augmentation settings.}
\end{figure}

\subsection{Ablation study: transport formulation}
To evaluate the impact of transport source design, we compare two Flow-Matching formulations under identical architecture and optimization settings: (i) noise-to-mask transport, where the source is sampled from Gaussian noise, and (ii) image-to-mask transport, where the source is the encoded image latent used in our main method. All other factors are held fixed, including VAE encoder/decoder, Flow-UNet backbone, augmentation policy, training epochs, and inference procedure.

Table~\ref{tab:transport_ablation} reports the cross-validation DSC comparison. The image-to-mask formulation achieves higher mean DSC than the noise-to-mask alternative in this cohort. Fold-level trends are consistent with the cohort-level mean (per-fold values can be added in the supplementary material if required by the final submission format).

\begin{table}
\caption{\label{tab:transport_ablation}Ablation of transport source formulation in Flow-Matching.}
\begin{center}
\begin{tabular}{lc}
\hline
Transport formulation & DSC (mean$\pm$std) \\
\hline
Noise-to-mask ($z_0\sim \mathcal{N}(0,I)$, $z_1=\mathrm{Enc}(m)$) & XX.XX $\pm$ XX.XX \\
Image-to-mask ($z_0=\mathrm{Enc}(I)$, $z_1=\mathrm{Enc}(m)$) & \textbf{XX.XX $\pm$ XX.XX} \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{VAE compression bottleneck analysis}
To isolate the latent compression effect from downstream segmentation error, we first measure pure VAE reconstruction fidelity by encoding and decoding ground-truth tumor masks. Figure~\ref{fig:vae_bottleneck} summarizes reconstruction Dice as a function of tumor size. Reconstruction fidelity decreases markedly for very small tumors, with a transition region around approximately 50 voxels.

We then compare this reconstruction envelope with final Flow-Matching prediction Dice on the same tumors. For small tumors, prediction Dice tracks the reconstruction ceiling closely, whereas for larger tumors a visible gap remains between reconstruction and prediction performance. This pattern indicates that part of the observed performance limit is attributable to latent-space representational fidelity rather than optimization instability alone. Quantitative thresholds and subgroup means should be reported directly in the final figure annotation or accompanying text once all measurements are finalized.

\begin{figure}
\centering
\fbox{\parbox{0.95\linewidth}{
\textbf{Placeholder for Figure 4 (VAE Bottleneck).}\\
Scatter plot (recommended two-panel layout):\\
Panel A: tumor volume (log scale) vs VAE reconstruction Dice.\\
Panel B: tumor volume (log scale) vs Flow-Matching prediction Dice, with reconstruction envelope reference.\\
Mark transition near 50 voxels.
}}
\caption{\label{fig:vae_bottleneck}Tumor-size-dependent fidelity under latent compression.}
\end{figure}

\subsection{Qualitative segmentation visualization}
Figure~\ref{fig:qualitative_results} presents representative qualitative cases comparing deterministic UNet and Flow-Matching outputs. Cases include an easy tumor with high overlap, an ambiguous-boundary tumor, and a small tumor near the latent-fidelity transition range.

Across representative examples, both methods perform similarly in well-defined tumors, while disagreement increases in boundary-ambiguous regions. For small tumors, both contour sharpness and topological completeness are reduced, consistent with the quantitative size-dependent trends in Section 3.5.

\begin{figure}
\centering
\fbox{\parbox{0.95\linewidth}{
\textbf{Placeholder for Figure 5 (Qualitative Results).}\\
Recommended columns: (A) BCT slice, (B) GT overlay, (C) UNet overlay, (D) Flow-Matching overlay.\\
Recommended rows: easy case, ambiguous-boundary case, small-tumor case.
}}
\caption{\label{fig:qualitative_results}Representative qualitative segmentation outputs.}
\end{figure}

\section{Discussion}
The comparison in Section 3.2 indicates that the proposed Flow-Matching framework provides a different performance profile from deterministic segmentation in small-cohort BCT. Although the absolute DSC improvement is modest, tumor-level detection sensitivity improves more clearly, suggesting that overlap-based metrics alone do not fully characterize practical behavior in sparse-target volumetric screening tasks. A plausible mechanism is that velocity-field learning in continuous time distributes supervision across interpolation states instead of forcing a single hard endpoint fit at every voxel. Under limited supervision, this can reduce brittle boundary decisions that arise when deterministic losses overfit sparse or noisy labels. This interpretation is consistent with the broader probabilistic-segmentation literature, where distribution learning is advantageous in anatomically ambiguous regions \cite{schmidt2023probabilistic, wu2024medsegdiff, rahman2023ambiguous}. Relative to diffusion-style segmentation, the same probabilistic framing is retained while inference remains computationally tractable through fixed-step ODE integration, in our case 25 Euler steps \cite{lipman2023flow, han2024semflow, zhao2025maisi}. For high-resolution 3D BCT, this tradeoff between uncertainty-aware output and inference efficiency is central to method feasibility.

The transport ablation in Section 3.4 provides additional evidence for this design choice. In a standard noise-to-mask formulation, the model must reconstruct all structural information from an unstructured Gaussian source before reaching a plausible tumor configuration. In contrast, image-to-mask transport starts from an encoded image latent and learns the residual displacement toward the mask manifold, which shortens the effective path length in latent space. Under the same architecture and optimization budget, this structured-source formulation yields higher DSC than noise-to-mask in our cohort. The result is consistent with the intuition behind SemFlow-style conditional transport \cite{han2024semflow}: when supervision is scarce and targets are small, reducing unnecessary transport complexity can improve optimization stability. The two formulations may also fail differently in practice. Noise-to-mask can generate anatomically implausible local structures, whereas image-to-mask may bias toward conservative updates (under-segmentation) when tumor evidence is weak. This tradeoff is important for interpreting method behavior beyond a single aggregate metric.

The augmentation ablation in Section 3.3 further suggests that ROI-centered 3D augmentation should be viewed as a training precondition rather than a minor optimization. In larger and more diverse datasets, augmentation often acts as a performance booster; in this setting, the no-augmentation variant can fail to converge stably, indicating insufficient geometric variation for robust optimization. This behavior is expected in BCT because tumor voxels occupy a tiny fraction of the full volume, and naive random sampling would overproduce background-only samples. By centering sampling on HU-derived candidates and perturbing local geometry through scale, rotation, and flipping, the pipeline preserves tumor context while materially increasing local appearance diversity. The design is also computationally aligned with high-resolution imaging: patch-level \texttt{F.grid\_sample} transforms avoid expensive whole-volume augmentation passes while remaining differentiable and compatible with GPU training. Compared with standard volumetric augmentation practice in nnU-Net-style pipelines and toolkits such as TorchIO and related multi-planar segmentation frameworks \cite{isensee2021nnu, perez-garcia2021torchio, perslev2019onetoall}, the present strategy is specifically adapted to the data-scale imbalance characteristic of emerging BCT cohorts.

An additional implementation detail is the latent-space label perturbation strategy. Binary labels encoded by a frozen VAE tend to occupy a narrow latent manifold, which can produce overly concentrated velocity targets and weak gradient diversity during training. Injecting bounded uniform perturbation before label encoding broadens the target distribution seen by the velocity regressor and acts as latent-space label smoothing. In practice, this stabilizes optimization without changing the semantic tumor support in voxel space. The uniform distribution is useful here because it preserves bounded perturbations around the binary support, avoiding occasional large excursions that can arise from unbounded Gaussian tails.

The most distinctive finding is the tumor-size-dependent VAE bottleneck in Section 3.5. The reconstruction analysis separates representational limits from downstream optimizer behavior by testing encode-decode fidelity directly on ground-truth masks. The observed fidelity drop for very small tumors supports the view that part of the segmentation ceiling is set upstream by latent compression resolution. With isotropic BCT resolution around 150\,$\mu$m, a tumor volume near 50 voxels corresponds to a characteristic sub-millimeter structure scale, where spatial downsampling can remove boundary detail needed for accurate topology recovery. This does not imply that Flow-Matching is uniquely constrained; rather, it indicates that any latent-space generative method inherits a compression-dependent upper bound when target structures approach the latent Nyquist limit. In that sense, the bottleneck should be interpreted as a physics-informed architectural constraint, not merely a model-training artifact. This observation complements latent diffusion findings in natural-image synthesis and latent-space medical segmentation \cite{rombach2022high, ngoc2025medseglatdiff} by showing that compression ratios acceptable for visual realism may be suboptimal for small-structure medical segmentation. For the BCT community, the practical implication is that latent compression ratio should be treated as a first-class design variable alongside network depth and loss design, especially when tumor detectability at sub-centimeter scale is a primary endpoint \cite{zhao2025maisi}.

This interpretation is closely related to a modeling tradeoff introduced by freezing a pre-trained MAISI AutoencoderKL during task-specific Flow-Matching training. Freezing the VAE is beneficial in the small-cohort setting because it leverages a broad medical-imaging representation learned at larger scale and reduces the risk of overfitting the encoder-decoder to a narrow institutional distribution. However, the same choice fixes the compression ratio at 8$\times$ and limits adaptation to BCT-specific small-target statistics. The bottleneck quantified in Section 3.5 can therefore be read as the cost of representation rigidity in exchange for training stability. A practical middle ground for future work may be selective adaptation (for example, decoder-side fine-tuning or low-rank adaptation) while preserving most of the foundation-model prior \cite{zhao2025maisi, rombach2022high, pinaya2022brain}.

These observations also motivate a metric-level discussion. Dice is a useful overlap index, but for small targets it is highly sensitive to minor boundary displacement. For example, mislabeling 10 boundary voxels in a 100-voxel tumor can reduce Dice by roughly 0.10, whereas the same absolute error in a 10,000-voxel structure produces a negligible change. In screening-oriented applications, the clinically dominant question is often tumor presence rather than contour perfection. Tumor-level detection sensitivity and false-positive behavior are therefore essential complements to DSC, and in some settings should be treated as primary endpoints. This perspective helps reconcile modest overlap gains with meaningful improvements in detection-oriented utility.

From a computational-physics viewpoint, the choice of Flow-Matching over diffusion is also practical for high-resolution BCT deployment. Sliding-window inference over large 3D volumes requires evaluating many patches, and per-patch integration cost scales directly with the number of function evaluations (NFE). A diffusion-style sampler with 200 steps can be computationally prohibitive at volume level, whereas 25-step Euler integration reduces NFE by approximately an order of magnitude while preserving conditional generation quality in this setting \cite{lipman2023flow, han2024semflow, zhao2025maisi, liu2023flow}. In addition, ODE-based integration is deterministic for a fixed input state, which supports reproducibility requirements in clinical and regulatory contexts.

Several limitations should be considered when interpreting these findings. First, the cohort size is limited and from a single-center acquisition setting, so external validity across scanners, protocols, and populations is not yet established \cite{shim2022fully, caballo2020mass}. Second, the comparative scope currently emphasizes a deterministic UNet baseline; broader benchmarking against additional strong baselines (for example, full nnU-Net pipelines and diffusion-segmentation variants) is still needed for a more exhaustive ranking. Third, the achieved DSC level remains below typical values reported in less challenging medical segmentation tasks, and the relative contributions of latent compression, annotation ambiguity, and data scarcity are not fully disentangled. Finally, this study focuses on technical feasibility and method characterization, not downstream diagnostic-outcome improvement; clinical utility therefore remains to be tested prospectively.

Future work will focus on reducing the latent-fidelity bottleneck while preserving tractable training and inference. Promising directions include lower-compression autoencoders, multi-scale latent pathways, and hybrid latent-pixel refinement schemes targeting high-frequency boundary retention. On the data side, larger multi-center BCT cohorts are needed to evaluate domain shift robustness and tumor-subtype behavior. On the benchmarking side, adding stronger deterministic and generative comparators will clarify which gains are specific to Flow-Matching and which reflect broader probabilistic modeling effects. For translational validation, prospective reader studies should quantify whether improved detection sensitivity and false-positive behavior translate into measurable clinical workflow benefit. Finally, extending from binary tumor segmentation to richer multi-class tissue characterization may better leverage the quantitative HU information available in BCT and strengthen links between physical imaging signals and generative inference.

\section{Conclusion}
This study establishes that generative segmentation with Flow-Matching is a practical option for high-resolution BCT in the small-cohort regime. The central empirical message is not only that voxel overlap improves, but that tumor-level detection improves more clearly than Dice alone suggests. The study also reveals that training behavior in this regime depends critically on how data variation is constructed: ROI-centered 3D geometric perturbation functions as a convergence condition under extreme class imbalance, not merely a minor performance enhancement. In addition, the observed tumor-size-dependent fidelity limit indicates that part of the attainable performance is governed by latent representation scale, independently of downstream predictor complexity.

Taken together, these findings imply that progress in BCT segmentation is best framed as a joint modeling-and-physics problem. The value of the present approach lies in coupling probabilistic inference with modality-specific constraints, so that algorithmic outputs remain consistent with known image ambiguity at tumor boundaries. More broadly, the results support a shift from pure leaderboard-style overlap optimization toward evaluation strategies that emphasize detection behavior and representational limits. At the same time, clinical translation requires cautious interpretation: larger multi-center cohorts, broader baseline comparisons, and prospective reader-impact studies are still needed before deployment-oriented claims can be made.

The most direct next step is to reduce the latent fidelity bottleneck while preserving computational tractability, for example through lower-compression or multi-scale representation designs. In parallel, future studies should quantify the practical value of improved tumor-level detection in real reading workflows. While voxel-level overlap remains constrained by latent spatial fidelity, the demonstrated detection sensitivity supports the feasibility of generative BCT segmentation as a screening-oriented decision-support direction.

\section*{References}
\bibliographystyle{vancouver}
\bibliography{references}



\end{document}


